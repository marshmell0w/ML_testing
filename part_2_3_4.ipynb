{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision is used for vision or computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you have to seperate training and testing \n",
    "train = datasets.MNIST(\"\", train = True, download = True, \n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train = False, download = True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size is how many we want to pass to our dataset, we have to batch for memory\n",
    "#and we use batching because we hope that it will generalize\n",
    "#passing it all at once will allow for more optimization\n",
    "\n",
    "#load the training data\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "\n",
    "#load the testing data\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([6, 5, 3, 4, 5, 6, 4, 9, 0, 5])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "#first we can reference the variable from the for loop\n",
    "#the data is worked out like this data[0] = the arrays, data[1] = the tensor output\n",
    "#then we need to reference the first of the array and of the tensor output\n",
    "#that would be data[0][0] and data[1][0]\n",
    "\n",
    "x, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#when we look at the shape\n",
    "data[0][0].shape\n",
    "\n",
    "#the shape is 1,28,28 which isn't a visual, its probably a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x278f3ac18e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "#if we had like 90% 3s the ML will learn to decrease loss by just guessing 3\n",
    "#that is because the loss function is determined on the output\n",
    "total  = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.87 %\n",
      "1: 11.24 %\n",
      "2: 9.93 %\n",
      "3: 10.22 %\n",
      "4: 9.74 %\n",
      "5: 9.04 %\n",
      "6: 9.86 %\n",
      "7: 10.44 %\n",
      "8: 9.75 %\n",
      "9: 9.92 %\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(\"{}:\".format(i),  round(counter_dict[i] / total * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    #we want to initialize the module\n",
    "    def __init__(self):\n",
    "        \n",
    "        #this is to pass through the features and paramaters of nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        #def fully conected layers\n",
    "        \n",
    "        #fc fully connected layers, 784 is 28 x 28, that is the flattened image\n",
    "        #we want 3 layers of 64 layes\n",
    "        \n",
    "        #input layer\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        \n",
    "        #these are more of our hidden layers they need to match output layer \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        #output layer\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    #how the data will flow through our network\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #passing through al of the layers, and they need the activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        #pass the data through without activation because its the output\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        #we use our loss function in the return statement, the dimension is the number of axis\n",
    "        #the dim says which number we want to pass to 1, dim1 is the output layer of tensors\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3626, -2.3818, -2.2147, -2.3641, -2.2679, -2.3574, -2.3485, -2.2513,\n",
       "         -2.2482, -2.2474]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#net.parameters is everything that is adjustable\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "EPOCHS = 3\n",
    "\n",
    "#when you have a big learning rate you never make steps big enough to test for a new optimization level\n",
    "#same with little steps they aren't big enough to try a new minima\n",
    "#we need the right size\n",
    "#we also can use decaying rate which starts with big jumps and then gets little jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2888, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    #data is a batch of feaures sets and labels\n",
    "    for data in trainset:\n",
    "        \n",
    "        #unpack the data, may be a tuple or a list\n",
    "        X, y = data\n",
    "        \n",
    "        #batching data decreases dataset\n",
    "        #batches help generalize \n",
    "        net.zero_grad()\n",
    "        \n",
    "        #this is the output\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        \n",
    "        #calculate loss metrics\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        #if your data is a vector use mean squared loss (for one hot vector)\n",
    "        \n",
    "        #now we want to backpropogate\n",
    "        loss.backward()\n",
    "        \n",
    "        #adjust the weights for us\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-61dfb0aef91d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;31m#log that correct prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mcorect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#we want to always keep track of how many tests we have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corect' is not defined"
     ]
    }
   ],
   "source": [
    "#keep track of how good things are\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#these are out of sample data points we don't want to optimize it\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        \n",
    "        #we want to compare the is the argmax (scalar value output)\n",
    "        for idx, i in enumerate(output):\n",
    "            \n",
    "            #if we have the right prediction\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                \n",
    "                #log that correct prediction\n",
    "                corect += 1\n",
    "        \n",
    "        #we want to always keep track of how many tests we have\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
